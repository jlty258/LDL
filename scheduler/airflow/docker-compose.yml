services:
  airflow-standalone:
    image: apache/airflow:2.9.0
    container_name: airflow-standalone
    command: standalone
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _AIRFLOW_WWW_USER_EMAIL: ${_AIRFLOW_WWW_USER_EMAIL:-airflow@example.com}
      _AIRFLOW_WWW_USER_FIRSTNAME: ${_AIRFLOW_WWW_USER_FIRSTNAME:-Airflow}
      _AIRFLOW_WWW_USER_LASTNAME: ${_AIRFLOW_WWW_USER_LASTNAME:-User}
    volumes:
      # Mount datawarehouse DAGs directory (primary source)
      - ../../datawarehouse/scheduler/airflow/dags:/opt/airflow/dags:ro
      - ../../data_volume/airflow/logs:/opt/airflow/logs
      - ../../data_volume/airflow/plugins:/opt/airflow/plugins
      - ../../data_volume/airflow/config:/opt/airflow/config
      # Mount SQL files directory for DAGs to access (separate mount point)
      - ../../datawarehouse/sql:/opt/airflow/sql:ro
    ports:
      - "8080:8080"
    restart: unless-stopped
    networks:
      - ldl-net

networks:
  ldl-net:
    external: true
    name: ldl-net
